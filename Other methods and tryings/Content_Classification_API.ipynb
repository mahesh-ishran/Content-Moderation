{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###**Basic info about the API**\n",
        "->API classifies text into content moderation categories listed below :\n",
        "<br>**'explicit_nudity', 'suggestive', 'violence', 'disturbing_content', 'rude_gestures', 'alcohol', 'drugs', 'tobacco', 'hate_speech', 'safe'**\n",
        "\n",
        "->It is built using **Meta Llama-3.1-8B-Instruct** model from Hugging Face\n",
        "\n",
        "->API framework: **FastAPI**\n",
        "\n",
        "###**Must to run this code succesfully**\n",
        "->change the run time type of colab from `CPU` to `T4-GPU` to load the Model successfully.\n",
        "\n",
        "->Model will not load properly on CPU.Session will crash.\n",
        "\n",
        "->this code will prompt for two type of tokens :\n",
        "* Hugging Face token : **........**\n",
        "* ngrok token : **..............**\n",
        "\n",
        "copy & paste them accordingly in prompt box\n",
        "\n",
        "###**<font color='red'>NOTE :**\n",
        "\n",
        "<font color='red'>**The ngrok public URL generated is temporary (only valid while the Colab notebook is running).**\n",
        "\n",
        "<font color='red'>**It is for development/demo purposes only.**\n",
        "\n",
        "<font color='red'>**For production deployment, use a paid ngrok plan or host the API on a dedicated server (AWS, GCP, Azure, etc.).**\n",
        "\n",
        "**Hugging Face token requires that the user has accepted Meta’s license agreement for the model on Hugging Face Hub.</font>**"
      ],
      "metadata": {
        "id": "4BrBbd1RUOJl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "20ef7a06",
        "outputId": "0113caf6-f8ca-4b12-997c-33d8213bd1af"
      },
      "source": [
        "!pip install pyngrok  bitsandbytes -U"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.3.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
            "Downloading pyngrok-7.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyngrok, bitsandbytes\n",
            "Successfully installed bitsandbytes-0.47.0 pyngrok-7.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cbda9QjeB4NZ"
      },
      "outputs": [],
      "source": [
        "import os, time, threading, re\n",
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "from fastapi import FastAPI, Request\n",
        "from pyngrok import ngrok\n",
        "import nest_asyncio, uvicorn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, getpass\n",
        "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"Paste your Hugging Face token (hidden): \")"
      ],
      "metadata": {
        "id": "nDG-xlLjajBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "vunsmHOOdKmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\", \"\")\n",
        "print(HF_TOKEN)"
      ],
      "metadata": {
        "id": "JCdCJbe2-oWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_labels = [\n",
        "    \"explicit_nudity\",\"suggestive\",\"violence\",\"disturbing_content\",\n",
        "    \"rude_gestures\",\"alcohol\",\"drugs\",\"tobacco\",\"hate_speech\",\"safe\"\n",
        "]\n",
        "labels_for_prompt = \", \".join(f\"'{l}'\" for l in valid_labels)\n",
        "print(labels_for_prompt)"
      ],
      "metadata": {
        "id": "fBc-N2kB-_Cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "gpu_name = torch.cuda.get_device_name(0) if device == \"cuda\" else \"CPU\"\n",
        "use_bf16 = device == \"cuda\" and any(k in gpu_name for k in [\"A100\",\"H100\",\"L4\"])\n",
        "print(f\"Device: {device} | GPU: {gpu_name} | bf16: {use_bf16}\")"
      ],
      "metadata": {
        "id": "otgSGsAl_Ylx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_generator():\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  gpu_name = torch.cuda.get_device_name(0) if device == \"cuda\" else \"CPU\"\n",
        "\n",
        "  use_bf16 = device == \"cuda\" and any(k in gpu_name for k in [\"A100\",\"H100\",\"L4\"])\n",
        "  print(f\"Device: {device} | GPU: {gpu_name} | bf16: {use_bf16}\")\n",
        "\n",
        "  if use_bf16:\n",
        "    return pipeline(\n",
        "      \"text-generation\",\n",
        "      model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "      model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
        "      device_map=\"auto\",\n",
        "      token=HF_TOKEN\n",
        "    )\n",
        "  else:\n",
        "    # 4-bit quantization for T4, small GPUs, or CPU\n",
        "    return pipeline(\n",
        "      \"text-generation\",\n",
        "      model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "      model_kwargs={\n",
        "          \"load_in_4bit\": True,\n",
        "          \"bnb_4bit_compute_dtype\": torch.bfloat16,\n",
        "          \"bnb_4bit_quant_type\": \"nf4\"\n",
        "      },\n",
        "      device_map=\"auto\",\n",
        "      token=HF_TOKEN\n",
        "    )"
      ],
      "metadata": {
        "id": "yXqcjA-T_DI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = make_generator()"
      ],
      "metadata": {
        "id": "GCOTj-5TA5Nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Helpers ----\n",
        "def normalize_labels(generated: str) -> str:\n",
        "  \"\"\"\n",
        "  Enforce that the output is only drawn from valid_labels. Falls back to 'safe' if nothing valid is detected.\"\"\"\n",
        "  text = generated.lower().strip()\n",
        "  # split by comma or newline\n",
        "  parts = [p.strip(\" '\\\"\\t.\").replace(\"-\", \"_\") for p in re.split(r\"[,\\n]+\", text)]\n",
        "  picked = [p for p in parts if p in valid_labels]\n",
        "  if not picked:\n",
        "    if \"safe\" in text:\n",
        "      picked = [\"safe\"]\n",
        "  picked = sorted(set(picked))\n",
        "  return \", \".join(picked) if picked else \"safe\""
      ],
      "metadata": {
        "id": "YXHBMnJqCX3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify(text: str) -> str:\n",
        "  messages = [\n",
        "    {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": (\n",
        "        \"You are an expert content moderator. Your task is to identify ALL \"\n",
        "        f\"applicable categories for the user's text from the following list: {labels_for_prompt}. \"\n",
        "        \"Your response MUST be a comma-separated list of the category names. \"\n",
        "        \"If none of the categories apply, respond with only the word 'safe'.\"\n",
        "      ),\n",
        "    },\n",
        "    {\"role\": \"user\", \"content\": f'Please classify the following text: \"{text}\"'},\n",
        "  ]\n",
        "  outputs = generator(messages, max_new_tokens=40, return_full_text=False)\n",
        "  raw = outputs[0][\"generated_text\"].strip()\n",
        "  print(raw)\n",
        "  return normalize_labels(raw)"
      ],
      "metadata": {
        "id": "Pswh_clxDdqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- FastAPI app ----\n",
        "app = FastAPI()\n",
        "\n",
        "@app.get(\"/health\")\n",
        "def health():\n",
        "  return {\"status\": \"ok\"}\n",
        "\n",
        "@app.post(\"/classify\")\n",
        "async def classify_content(request: Request):\n",
        "  try:\n",
        "    data = await request.json()\n",
        "    providers = data.get(\"message\", {}).get(\"catalog\", {}).get(\"bpp/providers\", [])\n",
        "    if not providers:\n",
        "      return {\n",
        "        \"type\": \"CATALOG-ERROR\",\n",
        "        \"code\": \"999999\",\n",
        "        \"path\": \"message.catalog.bpp/providers\",\n",
        "        \"message\": \"No providers found in input JSON\",\n",
        "        \"test_type\": \"recommendation\",\n",
        "      }\n",
        "\n",
        "    # Concatenate text fields from the first provider\n",
        "    provider = providers[0]\n",
        "    desc = provider.get(\"descriptor\", {}) or {}\n",
        "    text_list = desc.get(\"text\", []) or []\n",
        "    long_desc = desc.get(\"long_desc\", \"\") or \"\"\n",
        "    short_desc = desc.get(\"short_desc\", \"\") or \"\"\n",
        "\n",
        "    text_to_analyze = \" \".join(text_list + [long_desc, short_desc]).strip()\n",
        "    if not text_to_analyze:\n",
        "      return {\n",
        "        \"type\": \"CATALOG-ERROR\",\n",
        "        \"code\": \"999999\",\n",
        "        \"path\": \"message.catalog.bpp/providers[0].descriptor\",\n",
        "        \"message\": \"No text available for classification\",\n",
        "        \"test_type\": \"recommendation\",\n",
        "      }\n",
        "\n",
        "    t0 = time.time()\n",
        "    result = classify(text_to_analyze)\n",
        "    print(result)\n",
        "    t1 = time.time()\n",
        "\n",
        "    return {\n",
        "      \"type\": \"CATALOG\",\n",
        "      \"code\": \"111111\",\n",
        "      \"path\": \"message.catalog.bpp/providers[0].descriptor.text\",\n",
        "      \"message\": result,\n",
        "      \"test_type\": \"recommendation\",\n",
        "      \"inference_time\": f\"{t1 - t0:.2f} seconds\",\n",
        "    }\n",
        "\n",
        "  except Exception as e:\n",
        "    return {\n",
        "      \"type\": \"CATALOG-ERROR\",\n",
        "      \"code\": \"999999\",\n",
        "      \"path\": \"unknown\",\n",
        "      \"message\": f\"Exception: {e}\",\n",
        "      \"test_type\": \"recommendation\",\n",
        "    }"
      ],
      "metadata": {
        "id": "3V3NsR3UD4n0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_uvicorn():\n",
        "  uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
        "\n",
        "# Start the API server (background thread)\n",
        "server_thread = threading.Thread(target=run_uvicorn, daemon=True)\n",
        "server_thread.start()"
      ],
      "metadata": {
        "id": "ccdusv1-bGjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "public_url = None\n",
        "os.environ[\"ngrok_TOKEN\"] = getpass.getpass(\"Paste your ngrok token (hidden): \")\n",
        "NGROK_AUTH = os.environ.get(\"ngrok_TOKEN\", \"\")\n",
        "print(NGROK_AUTH)\n",
        "try:\n",
        "  if NGROK_AUTH:\n",
        "    ngrok.set_auth_token(NGROK_AUTH)\n",
        "  public_url = ngrok.connect(8000, \"http\").public_url\n",
        "  print(\"Public URL:\", public_url)\n",
        "except Exception as e:\n",
        "  print(\"Ngrok not started (you can ignore this if you don't need external access):\", e)"
      ],
      "metadata": {
        "id": "E3B5O3J9iTVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**-----------------------------------------trying the api-----------------------**"
      ],
      "metadata": {
        "id": "s2eKFNFZl3t2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "payload = {\n",
        "  \"message\": {\n",
        "    \"catalog\": {\n",
        "      \"bpp/providers\": [\n",
        "        {\n",
        "          \"descriptor\": {\n",
        "            \"text\": [\n",
        "              \"He was drunk. He drew his sword and charged, intending to cut them down where they stood.\"\n",
        "            ],\n",
        "            \"long_desc\": \"\",\n",
        "            \"short_desc\": \"\"\n",
        "          },\n",
        "          \"id\": \"1268365919\"\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "# Local call\n",
        "r_local = requests.post(\"http://127.0.0.1:8000/classify\", json=payload, timeout=60)\n",
        "print(\"Local response:\", r_local.json())\n",
        "\n",
        "# Public call (if you got an ngrok URL)\n",
        "if 'public_url' in globals() and public_url:\n",
        "    r_public = requests.post(f\"{public_url}/classify\", json=payload, timeout=60)\n",
        "    print(\"Public response:\", r_public.json())\n"
      ],
      "metadata": {
        "id": "OTXIuEIrk2uo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}